Amazon SageMaker's **Linear Learner** algorithm is a supervised learning method designed to address both classification and regression tasks. It achieves this by learning a linear function (for regression) or a linear threshold function (for classification) that maps input features to the target variable. This algorithm is versatile, supporting binary classification, multiclass classification, and regression problems.

**Training Input:**

Linear Learner accepts data in two primary formats:

- **RecordIO-wrapped protobuf:** This format supports only Float32 data.

- **CSV:** In this format, the first column is assumed to be the label (target variable), and subsequent columns represent features.

Both **File mode** and **Pipe mode** are supported for data ingestion, allowing flexibility in how data is streamed or loaded during training.

**Usage:**

The typical workflow for using Linear Learner involves several key steps:

1. **Preprocessing:**

   - **Normalization:** It's crucial to normalize training data to ensure all features contribute equally to the model. Linear Learner can automatically perform this normalization.

   - **Shuffling:** To enhance training effectiveness, input data should be shuffled to prevent the model from learning spurious patterns.

2. **Training:**

   - **Optimization:** The algorithm employs stochastic gradient descent (SGD) and allows users to select from various optimization algorithms, such as Adam, AdaGrad, or standard SGD.

   - **Parallel Model Training:** Linear Learner can train multiple models in parallel, each with different hyperparameter configurations, to identify the most optimal model.

   - **Regularization:** Users can tune L1 and L2 regularization parameters to prevent overfitting and improve model generalization.

3. **Validation:**

   - The algorithm evaluates models against a validation set (if provided) to select the best-performing model based on predefined criteria.

**Important Hyperparameters:**

Key hyperparameters that influence the performance of the Linear Learner include:

- **balance_multiclass_weights:** Ensures each class has equal importance in the loss function, which is particularly useful for imbalanced datasets.

- **learning_rate:** Controls the step size during optimization.

- **mini_batch_size:** Determines the number of samples per batch during training.

- **l1 (L1 regularization):** Adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function, promoting sparsity.

- **wd (Weight Decay or L2 regularization):** Adds a penalty equal to the square of the magnitude of coefficients to the loss function, discouraging large weights.îˆ†

- **target_precision:** When used with `binary_classifier_model_selection_criteria` set to `recall_at_target_precision`, it maintains precision at a specified value while maximizing recall.

- **target_recall:** When used with `binary_classifier_model_selection_criteria` set to `precision_at_target_recall`, it maintains recall at a specified value while maximizing precision.

A comprehensive list of hyperparameters and their descriptions can be found in the [SageMaker Linear Learner Hyperparameters documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html).

**Instance Types:**

Linear Learner supports training on both CPU and GPU instances:

- **Training:**

  - **CPU Instances:** Suitable for smaller datasets or less computationally intensive tasks.

  - **GPU Instances:** Beneficial for larger datasets and more complex models. However, it's important to note that multi-GPU configurations do not necessarily lead to performance improvements for this algorithm.

For detailed information on instance recommendations, refer to the [SageMaker Linear Learner Algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html).
