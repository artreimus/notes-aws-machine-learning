# Distributed Training in SageMaker

### 1. Topic

**Distributed Training**: Scaling individual model training across multiple GPUs and instances.

---

### 2. Why It Matters

- **Singleâ€‘instance limits**: Even the largest ML instances (e.g.,Â `ml.p4d.24xlarge` with 8Â GPUs) may not hold or train extremely large models (billionsÂ + parameters).
- **Faster convergence**: Parallelism can reduce wallâ€‘clock training time.
- **Cost vs. efficiency tradeâ€‘off**: Larger single instances often outperform multiple smaller ones due to lower communication overhead.

---

### 3. Types of Parallelism

| Parallelism Type      | What Is Parallelized         | When to Use                                                |
| --------------------- | ---------------------------- | ---------------------------------------------------------- |
| **Job Parallelism**   | Multiple independent jobs    | Training different models or hyperparameter trials in bulk |
| **Data Parallelism**  | Training data across GPUs    | Same model & dataset split across devices/nodes            |
| **Model Parallelism** | Model parameters across GPUs | Model too large for one GPUâ€™s memory                       |

---

### 4. Distributed Data Parallelism (DDP)

#### ðŸ”¹ Concept

- **AllReduce** collective: each worker computes gradients on its data shard, then gradients are averaged (reduced) across all workers.
- **AllGather** collective: when needed, workers share intermediate data or metadata.

#### ðŸ”¹ SageMaker SMDDP

- Built on AWS Custom Collectives (optimized for EC2 network)
- **Enable in Estimator**:

  ```python
  from sagemaker.pytorch import PyTorch

  estimator = PyTorch(
      â€¦,
      distribution={
        'smdistributed_dataparallel': {'enabled': True}
      }
  )
  ```

- **Benefits**: Lower allâ€‘reduce overhead than native PyTorch DDP; works seamlessly with mixedâ€‘precision and large batch sizes.

#### ðŸ”¹ Alternative Frameworks

- **PyTorch native DDP**: `distribution={'pytorchx': {'enabled': True}}`
- **TorchRun** (`torch.distributed`): MPIâ€‘style, requires `ml.p3`/`ml.p4`/`ml.trn1` instances
- **Horovod**: MPI-based framework; enable via `sagemaker.horovod.Horovod` estimator
- **DeepSpeed**: Microsoftâ€™s library for ultraâ€‘scale (ZeRO optimizations), often used via Hugging Face on SageMaker

---

### 5. Distributed Model Parallelism (MPP)

#### ðŸ”¹ Concept

Split a single modelâ€™s parameters and states across GPUs/nodes when it cannot fit into one device.

#### ðŸ”¹ SageMaker SM Model Parallelism

- **Enable in Estimator**:
  ```python
  estimator = PyTorch(
      â€¦,
      distribution={
        'smdistributed_modelparallel': {
          'enabled': True,
          'parameters': {
            'microbatches': 4,
            'placement_strategy': 'spread',
            // additional MPP configs
          }
        }
      }
  )
  ```
- **Key Techniques**:
  1. **Optimization State Sharding**
     - Shard optimizer states (weights, momentum) across GPUs
     - Requires stateful optimizers (e.g., Adam, AdamW)
  2. **Activation Checkpointing**
     - Discard intermediate activations during forward pass
     - Recompute them during backward pass to save memory
  3. **Activation Offloading**
     - Swap checkpointed activations to CPU memory, freeing GPU RAM
     - Trades off dataâ€‘transfer time for reduced GPU memory footprint
  4. **Sharded Data Parallelism**
     - Combines optimizer sharding with parameter & gradient sharding
     - Further reduces perâ€‘GPU memory usage and scales to larger clusters

---

### 6. Choosing Parallelism Strategy

1. **Max out single instance** first (e.g., `ml.p4d.24xlarge` with 8 GPUs).
2. If model fits but training is slow â†’ **Data Parallelism** (SMDDP or native DDP).
3. If model doesnâ€™t fit GPU memory â†’ **Model Parallelism** (SMMP) Â± sharded data parallelism.
4. For extreme scale or custom needs â†’ consider **Horovod** or **DeepSpeed**.

---

### 7. Best Practices & Tips

- **Minimize crossâ€‘node communication**: Increase batch sizes and microbatch counts where possible.
- **Profile overhead** using SageMaker Debuggerâ€™s `ProfilerReport` and **network metrics**.
- **Balance compute vs. memory**: Use activation checkpointing/offloading only when memory is the critical bottleneck.
- **Monitor costs**: Multiâ€‘node jobs incur both compute and interconnect costs; choose instance types wisely.
- **Stay within compatibility**: SMDDP and SMMP do **not** work with the deprecated Training Compiler.

---

### 8. Examâ€‘Focused Quick Triggers

- **AllReduce** = averaging gradients across workers (data parallelism).
- **AllGather** = collecting tensors/metadata among nodes.
- **SMDDP** = SageMakerâ€™s optimized data parallel library.
- **SMMP** = SageMakerâ€™s model parallel library (sharding + checkpointing + offload).
- **Horovod**/**DeepSpeed** = thirdâ€‘party options for MPI or ZeROâ€‘based scaling.
- **Single large instance** vs. **multiâ€‘instance**: always try a bigger instance before distributing.

---

With these distributed training techniques, youâ€™ll be ready to architect SageMaker jobs that train the largest modern modelsâ€”exactly what the exam will test. Good luck!
